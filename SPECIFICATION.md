## PDFチャットボットWebアプリ仕様書

この仕様書は、PDFの内容に基づいたRAG（Retrieval-Augmented Generation）チャットボットを作成するWebアプリケーションの設計と機能について詳細に記述したものです。AIエージェントによる実装を考慮し、処理フローやデータ構造を明確に定義します。

-----

### 1\. アプリケーション概要

#### 1.1. 目的

機密性の高い文書（社内規定、法務書類など）や、査読前の学術論文など、外部のAPIに送信したくない文書の内容を、ローカル環境で動作する大規模言語モデル（LLM）を用いて安全かつ対話的に読解・分析することを支援します。

#### 1.2. ターゲットユーザー

  * 研究者、学生
  * 企業の法務・知財担当者
  * 機密情報を扱うナレッジワーカー全般

#### 1.3. コアコンセプト

1.  **セキュアな環境**: Ollamaを通じてローカルLLMを利用するため、機密情報が外部に漏洩するリスクを最小限に抑えます。
2.  **高精度な回答生成**: RAGアーキテクチャにより、アップロードされたPDFの内容に基づいた、正確で根拠のある回答を生成します。
3.  **柔軟なカスタマイズ**: ユーザーがLLM、Embeddingモデル、Rerankerを自由に選択・変更できる設定機能を提供します。

-----

### 2\. 機能要件

#### 2.1. PDF処理機能

  * **PDFアップロード**: ユーザーはローカルデバイスからPDFファイルをアップロードできます。ドラッグ＆ドロップにも対応します。
  * **テキスト抽出**: アップロードされたPDFからテキスト情報を抽出します。
  * **チャンク分割**: 抽出したテキストを、意味的なまとまりを維持しつつ、適切な長さのチャンク（断片）に分割します。分割サイズやオーバーラップは設定画面で調整可能です。
  * **ベクトル化と保存**: 各チャンクを、ユーザーが選択したEmbeddingモデルを用いてベクトル化し、FAISS（Facebook AI Similarity Search）などの効率的なベクトルストアにインデックス化して保存します。このプロセスはサーバーサイドで非同期に実行されます。

#### 2.2. チャット機能

  * **対話インターフェース**: ユーザーが質問を入力し、ボットからの回答を受け取るためのUIを提供します。
  * **RAGパイプライン**:
    1.  ユーザーの質問をベクトル化します。
    2.  ベクトルストアから、質問ベクトルと類似度の高いテキストチャンクを複数検索（Retrieve）します。
    3.  （オプション）選択したRerankerモデルを用いて、検索結果を質問との関連性がより高い順に並べ替えます。
    4.  LLMに渡すプロンプトを生成します。プロンプトには、元の質問と、関連情報として取得した上位N個のチャンクが含まれます。
    5.  LLMが生成（Generate）した回答を、リアルタイムにストリーミング表示します。
  * **引用表示**: 回答を生成する際に参照したPDF内のチャンク（根拠）を、回答と共に表示する機能を持ちます。これにより、ユーザーは情報の正当性を容易に確認できます。

#### 2.3. 会話管理機能

  * **会話履歴の永続化**: すべての会話（ユーザーの質問とボットの回答）は自動的に保存されます。
  * **セッション管理**:
      * PDFごとに新しい会話セッションが開始されます。
      * 過去の会話セッションを一覧で表示し、選択して再開できます。
      * セッションの名前変更、削除機能を提供します。
  * **エクスポート機能**:
      * 現在の会話履歴をJSON形式でエクスポートできます。
      * 現在の会話履歴をMarkdown形式でエクスポートできます。

#### 2.4. 設定機能

  * **Ollama設定**:
      * OllamaサーバーのAPIエンドポイントURLを設定できます。
      * 利用可能なLLMモデルをOllama APIから動的に取得し、ドロップダウンリストで選択できます。
      * 利用可能なEmbeddingモデルをOllama APIから動的に取得し、ドロップダウンリストで選択できます。
      * 利用可能なRerankerモデルを選択できます（別途インストール・設定が必要）。
  * **RAG設定**:
      * テキストチャンクのサイズとオーバーラップ率を設定できます。
      * LLMに渡す類似チャンクの数（Top-K）を調整できます。
  * **コンテキスト管理設定（後述）**: 会話の記憶に関する設定を行えます。

#### 2.5. 【提案機能】コンテキスト管理

LLMのコンテキスト長には制限があるため、無限に過去の対話を記憶することはできません。そこで、以下の段階的なコンテキスト管理戦略を提案します。

  * **基本戦略 (Sliding Window)**: 直近の `N` 回の対話（ユーザーの質問＋ボットの回答）のみをコンテキストとしてLLMに渡します。`N` は設定画面で変更可能です。シンプルで実装が容易です。
  * **高度な戦略 (Vector-based Memory)**:
    1.  これまでの会話履歴も、質問や回答ごとにチャンク化し、ベクトル化して専用の「会話メモリ」ベクトルストアに保存します。
    2.  新しい質問が来た際に、PDFのベクトルストアから関連情報を検索するのと**同時に**、「会話メモリ」ベクトルストアからも関連性の高い過去の対話を検索します。
    3.  PDFから取得したチャンクと、過去の対話から取得したチャンクの両方をコンテキストとしてLLMに渡し、より文脈を理解した回答を生成させます。
    <!-- end list -->
      * **利点**: 「前の会話で言った〇〇についてだけど…」のような、文脈に依存した質問に極めて強くなります。

-----

### 3\. 画面仕様

  \#\#\#\# 3.1. メインタブ
アプリケーションのメイン画面です。画面は左右に2分割されます。

  * **左ペイン: PDFビューアエリア**
      * **初期表示**: PDFのアップロードを促すインターフェース（ファイル選択ボタン、ドラッグ＆ドロップエリア）。
      * **アップロード後**: アップロードされたPDFのプレビューが表示されます。ページ送り機能、拡大・縮小機能を持ちます。チャットボットが回答の根拠として示した箇所をハイライト表示する機能も備えます。
  * **右ペイン: チャットエリア**
      * **ヘッダー**: 現在の会話セッション名、エクスポートボタン（JSON, MD）、削除ボタン。
      * **会話表示**: ユーザーの質問とボットの回答が時系列に表示されます。
      * **入力フォーム**: ユーザーが質問を入力するテキストエリアと送信ボタン。

#### 3.2. 設定タブ

アプリケーション全体の設定を行う画面です。

  * **Ollama Connection**:
      * `API Endpoint URL`: テキスト入力欄
      * `Test Connection` ボタン
  * **Model Selection**:
      * `LLM Model`: ドロップダウンリスト
      * `Embedding Model`: ドロップダウンリスト
      * `Reranker Model`: ドロップダウンリスト
  * **RAG Parameters**:
      * `Chunk Size`: スライダー or 数値入力
      * `Chunk Overlap`: スライダー or 数値入力
      * `Top-K`: スライダー or 数値入力
  * **Context Management**:
      * `Strategy`: ドロップダウンリスト (`Sliding Window`, `Vector-based Memory`)
      * `Window Size (N turns)`: スライダー or 数値入力（Sliding Window選択時）
  * **下部**: `Save` ボタン、`Cancel` ボタン。

-----

### 4\. 技術仕様

#### 4.1. アーキテクチャ

  * **フロントエンド**: React (Vite) / Vue.js / Svelte などのモダンなJavaScriptフレームワーク。
  * **バックエンド**: Python。Webフレームワークとして **FastAPI** を推奨（非同期処理に強く、API定義が容易なため）。
  * **通信**: フロントエンドとバックエンド間は、リアルタイム性を確保するため **WebSocket** をチャット機能に利用。その他は通常のHTTPリクエスト。

#### 4.2. 主要ライブラリ（バックエンド）

  * **Webフレームワーク**: `fastapi`, `uvicorn`
  * **Ollama連携**: `ollama`
  * **PDF処理**: `pypdf` or `pymupdf`
  * **テキスト分割**: `langchain` or `llama-index` の `TextSplitter`
  * **ベクトルストア**: `faiss-cpu` (or `faiss-gpu`), `numpy`
  * **WebSocket**: `websockets`

#### 4.3. APIエンドポイント（例）

| Method | Endpoint                    | 説明                                                              |
| :----- | :-------------------------- | :---------------------------------------------------------------- |
| `POST` | `/upload`                   | PDFファイルをアップロードし、テキスト抽出とベクトル化を開始する。 |
| `GET`  | `/status/{job_id}`          | PDF処理ジョブの進捗状況を問い合わせる。                             |
| `WS`   | `/chat/{session_id}`        | 指定されたセッションでチャット通信を行うWebSocketエンドポイント。   |
| `GET`  | `/sessions`                 | すべての会話セッションのリストを取得する。                          |
| `POST` | `/sessions`                 | 新しい会話セッションを作成する。                                  |
| `DELETE`| `/sessions/{session_id}`    | 指定されたセッションを削除する。                                  |
| `GET`  | `/export/{session_id}/{format}`| 会話履歴を指定の形式（json, md）でエクスポートする。           |
| `GET`  | `/settings`                 | 現在の設定を取得する。                                            |
| `POST` | `/settings`                 | 設定を更新する。                                                  |
| `GET`  | `/ollama/models`            | Ollamaから利用可能なモデルリストを取得する。                      |

#### 4.4. データ構造（例）

  * **会話メッセージ (JSON)**

    ```json
    {
      "role": "user" | "assistant",
      "content": "...",
      "timestamp": "2025-07-06T12:00:00Z",
      "references": [ // roleがassistantの場合
        {
          "chunk_id": "...",
          "text": "...",
          "page_number": 5
        }
      ]
    }
    ```

  * **セッション情報 (JSON)**

    ```json
    {
      "session_id": "uuid-...",
      "session_name": "論文Aに関する対話",
      "pdf_filename": "paper_a.pdf",
      "created_at": "...",
      "updated_at": "..."
    }
    ```

-----

### 5\. 非機能要件

  * **パフォーマンス**: PDFのベクトル化処理はCPU負荷が高いため、バックグラウンドで非同期に実行し、UIをブロックしないこと。
  * **エラーハンドリング**: Ollamaサーバーに接続できない場合や、PDFの処理に失敗した場合などに、適切なエラーメッセージをユーザーに表示すること。
  * **永続化**: 会話履歴や設定は、アプリケーションを閉じても保持されるように、サーバーサイドのファイルや軽量なデータベース（SQLiteなど）に保存すること。
  * **セキュリティ**: ローカル動作が前提だが、ファイルパスのトラバーサルなど、基本的なセキュリティ対策は考慮すること。

### 6\. ディレクトリ構成

```
PDFRAGChat/
├── backend/
│   ├── app/
│   │   ├── api/
│   │   │   ├── __init__.py
│   │   │   ├── chat.py         # WebSocketチャットエンドポイント
│   │   │   ├── documents.py    # PDFアップロード、処理エンドポイント
│   │   │   └── settings.py     # 設定管理エンドポイント
│   │   ├── core/
│   │   │   ├── __init__.py
│   │   │   ├── config.py       # 設定の読み込み・管理
│   │   │   ├── pdf_processor.py# PDFテキスト抽出・チャンク化
│   │   │   ├── rag_pipeline.py # RAG処理のコアロジック
│   │   │   └── vector_store.py # ベクトルストア管理(FAISS)
│   │   ├── models/
│   │   │   ├── __init__.py
│   │   │   └── schemas.py      # Pydanticモデル(APIのデータ構造)
│   │   ├── __init__.py
│   │   └── main.py             # FastAPIアプリケーションのエントリーポイント
│   ├── data/                   # (Git管理外) アップロードPDF、ベクトルDB、会話ログ
│   │   ├── pdfs/
│   │   ├── vector_stores/
│   │   └── sessions/
│   ├── requirements.txt        # Python依存ライブラリ
│   └── .env.example            # 環境変数設定ファイル例
│
├── frontend/
│   ├── src/
│   │   ├── components/
│   │   │   ├── ChatWindow.jsx  # チャットUIコンポーネント
│   │   │   ├── PdfViewer.jsx   # PDFビューアコンポーネント
│   │   │   └── SettingsForm.jsx# 設定フォームコンポーネント
│   │   ├── hooks/
│   │   │   └── useChat.js      # チャット(WebSocket)関連のカスタムフック
│   │   ├── pages/
│   │   │   ├── MainPage.jsx    # メインページのレイアウト
│   │   │   └── SettingsPage.jsx# 設定ページのレイアウト
│   │   ├── services/
│   │   │   └── api.js          # バックエンドAPIと通信する関数群
│   │   ├── App.jsx             # アプリケーションのルーティング
│   │   └── main.jsx            # Reactアプリのエントリーポイント
│   ├── package.json
│   └── vite.config.js
│
└── .gitignore
```